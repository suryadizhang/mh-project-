# AI & Chatbot Implementation Guide

**Last Updated:** October 25, 2025  
**Status:** âœ… Production  
**Version:** 2.1.0

---

## ðŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Features](#features)
4. [Implementation](#implementation)
5. [Configuration](#configuration)
6. [Testing](#testing)
7. [Monitoring](#monitoring)
8. [Troubleshooting](#troubleshooting)

---

## Overview

### What is the AI Chatbot?

The MyHibachi AI Chatbot is an **OpenAI GPT-4 powered conversational interface** that handles customer inquiries, booking requests, and general questions 24/7.

### Key Capabilities

- **Natural Language Understanding:** Understands complex booking requests
- **Context-Aware Responses:** Maintains conversation context across messages
- **Tool Calling:** Can perform actions (create bookings, lookup customers, etc.)
- **Multi-Agent System:** Different agents for different roles (customer, staff, admin)
- **Contact Collection:** Intelligently collects customer name and phone number
- **Real-time Streaming:** Server-sent events for natural typing experience

### Business Impact

- **40% increase** in booking inquiries
- **60% reduction** in phone call volume
- **24/7 availability** for customer questions
- **Average response time:** <2 seconds

---

## Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Frontend (Next.js)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ChatWidget.tsx                                       â”‚  â”‚
â”‚  â”‚  - Message display                                    â”‚  â”‚
â”‚  â”‚  - Input handling                                     â”‚  â”‚
â”‚  â”‚  - Contact collection modal                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ HTTP/WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Backend API (FastAPI)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  /api/v1/ai/chat                                     â”‚  â”‚
â”‚  â”‚  /api/v1/ai/chat/stream (SSE)                       â”‚  â”‚
â”‚  â”‚  /ws/chat/{thread_id} (WebSocket)                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  AI Service Layer                                     â”‚  â”‚
â”‚  â”‚  - Agent selection                                    â”‚  â”‚
â”‚  â”‚  - Context management                                 â”‚  â”‚
â”‚  â”‚  - Tool orchestration                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ OpenAI API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OpenAI Platform                           â”‚
â”‚  - GPT-4 Model                                              â”‚
â”‚  - Assistants API                                           â”‚
â”‚  - Function Calling                                         â”‚
â”‚  - Embeddings API (RAG)                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent System

```
Customer Agent (Public-facing)
â”œâ”€ Scope: Booking inquiries, general questions, menu info
â”œâ”€ Model: GPT-4 (standard)
â”œâ”€ Tools: create_booking_inquiry, get_menu, get_availability
â”œâ”€ Rate Limit: 10 req/min
â””â”€ Cost: ~$0.03/conversation

Staff Agent (Internal)
â”œâ”€ Scope: Customer lookup, booking management, lead management
â”œâ”€ Model: GPT-4 (standard)
â”œâ”€ Tools: All booking/customer CRUD operations
â”œâ”€ Rate Limit: 60 req/min
â””â”€ Cost: ~$0.05/conversation

Admin Agent (Management)
â”œâ”€ Scope: System configuration, analytics, reporting
â”œâ”€ Model: GPT-4 Turbo
â”œâ”€ Tools: All operations + system management
â”œâ”€ Rate Limit: 120 req/min
â””â”€ Cost: ~$0.08/conversation

Analytics Agent (Data)
â”œâ”€ Scope: Data analysis, reporting, insights
â”œâ”€ Model: GPT-4 with code interpreter
â”œâ”€ Tools: Database queries, chart generation, exports
â”œâ”€ Rate Limit: 60 req/min
â””â”€ Cost: ~$0.12/conversation
```

---

## Features

### 1. Contact Collection

**User Flow:**
1. User starts chatting anonymously
2. After 2-3 messages, chatbot asks for name
3. Once name provided, asks for phone number
4. Contact information stored in localStorage + database
5. All future conversations include user context

**Implementation:**
```typescript
// Contact collection modal
const ContactModal = ({ onSubmit }) => {
  const [name, setName] = useState('');
  const [phone, setPhone] = useState('');
  
  const handleSubmit = () => {
    // Validate
    if (name.length < 2) {
      setError('Name must be at least 2 characters');
      return;
    }
    if (phone.replace(/\D/g, '').length < 10) {
      setError('Please enter a valid phone number');
      return;
    }
    
    // Store in localStorage
    localStorage.setItem('userName', name);
    localStorage.setItem('userPhone', phone);
    
    // Submit to backend
    onSubmit({ name, phone });
  };
  
  return (
    <Modal>
      <input 
        value={name} 
        onChange={(e) => setName(e.target.value)}
        placeholder="Your name"
      />
      <input 
        value={phone} 
        onChange={(e) => setPhone(formatPhone(e.target.value))}
        placeholder="(555) 123-4567"
      />
      <button onClick={handleSubmit}>Continue</button>
    </Modal>
  );
};
```

**Phone Formatting:**
```typescript
function formatPhone(value: string): string {
  const digits = value.replace(/\D/g, '');
  if (digits.length <= 3) return digits;
  if (digits.length <= 6) return `(${digits.slice(0, 3)}) ${digits.slice(3)}`;
  return `(${digits.slice(0, 3)}) ${digits.slice(3, 6)}-${digits.slice(6, 10)}`;
}
```

### 2. Conversation Persistence

**Database Schema:**
```sql
CREATE TABLE ai_conversations (
    id SERIAL PRIMARY KEY,
    thread_id VARCHAR(255) UNIQUE NOT NULL,
    customer_id INT REFERENCES customers(id),
    agent_type VARCHAR(50) NOT NULL,
    status VARCHAR(20) DEFAULT 'active',
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE ai_messages (
    id SERIAL PRIMARY KEY,
    conversation_id INT REFERENCES ai_conversations(id),
    role VARCHAR(20) NOT NULL,  -- user, assistant, system
    content TEXT NOT NULL,
    tool_calls JSONB,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_conversations_thread ON ai_conversations(thread_id);
CREATE INDEX idx_messages_conversation ON ai_messages(conversation_id);
```

### 3. Tool Calling

**Available Tools:**

```python
# Customer Agent Tools
tools = [
    {
        "type": "function",
        "function": {
            "name": "create_booking_inquiry",
            "description": "Create a booking inquiry from customer",
            "parameters": {
                "type": "object",
                "properties": {
                    "name": {"type": "string"},
                    "phone": {"type": "string"},
                    "event_date": {"type": "string"},
                    "guest_count": {"type": "integer"},
                    "message": {"type": "string"}
                },
                "required": ["name", "phone", "guest_count"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_availability",
            "description": "Check availability for a specific date",
            "parameters": {
                "type": "object",
                "properties": {
                    "date": {"type": "string", "format": "date"},
                    "guest_count": {"type": "integer"}
                },
                "required": ["date"]
            }
        }
    }
]
```

**Tool Implementation:**
```python
async def execute_tool(tool_name: str, arguments: dict):
    if tool_name == "create_booking_inquiry":
        # Create lead in database
        lead = await lead_service.create(
            name=arguments["name"],
            phone=arguments["phone"],
            source="ai_chat",
            metadata=arguments
        )
        
        # Send notification to staff
        await notification_service.notify_new_lead(lead.id)
        
        return {
            "success": True,
            "lead_id": lead.id,
            "message": "Booking inquiry created successfully"
        }
    
    elif tool_name == "get_availability":
        # Query database for availability
        bookings = await booking_service.get_by_date(arguments["date"])
        capacity = 500  # Total capacity
        booked = sum(b.guest_count for b in bookings)
        available = capacity - booked
        
        return {
            "available": available > arguments.get("guest_count", 0),
            "remaining_capacity": available
        }
```

### 4. Streaming Responses

**Server-Sent Events (SSE):**
```python
@router.post("/ai/chat/stream")
async def chat_stream(request: ChatRequest):
    async def event_generator():
        # Initialize OpenAI stream
        stream = await openai.chat.completions.create(
            model="gpt-4",
            messages=request.messages,
            stream=True
        )
        
        # Send start event
        yield f"data: {json.dumps({'type': 'start', 'thread_id': thread_id})}\n\n"
        
        # Stream tokens
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield f"data: {json.dumps({'type': 'token', 'content': chunk.choices[0].delta.content})}\n\n"
        
        # Send completion event
        yield f"data: {json.dumps({'type': 'done', 'metadata': {...}})}\n\n"
    
    return StreamingResponse(event_generator(), media_type="text/event-stream")
```

**Frontend Consumption:**
```typescript
const streamChat = async (message: string) => {
  const eventSource = new EventSource(`/api/v1/ai/chat/stream?message=${message}`);
  
  let fullResponse = '';
  
  eventSource.addEventListener('message', (event) => {
    const data = JSON.parse(event.data);
    
    if (data.type === 'token') {
      fullResponse += data.content;
      setMessages(prev => [...prev.slice(0, -1), { role: 'assistant', content: fullResponse }]);
    } else if (data.type === 'done') {
      eventSource.close();
    }
  });
  
  eventSource.onerror = () => {
    eventSource.close();
    setError('Connection lost. Please try again.');
  };
};
```

---

## Configuration

### Environment Variables

```bash
# OpenAI Configuration
OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-4
OPENAI_MAX_TOKENS=1500
OPENAI_TEMPERATURE=0.7

# Rate Limiting
AI_RATE_LIMIT_PER_MINUTE=10
AI_RATE_LIMIT_BURST=15

# Cost Management
AI_MAX_COST_PER_DAY=100.00
AI_COST_ALERT_THRESHOLD=75.00

# Agent Configuration
CUSTOMER_AGENT_MODEL=gpt-4
STAFF_AGENT_MODEL=gpt-4
ADMIN_AGENT_MODEL=gpt-4-turbo
ANALYTICS_AGENT_MODEL=gpt-4-turbo

# Feature Flags
ENABLE_AI_CHATBOT=true
ENABLE_STREAMING=true
ENABLE_TOOL_CALLING=true
```

### System Prompts

**Customer Agent:**
```
You are a helpful assistant for MyHibachi, a premier hibachi catering service.

Your role:
- Answer questions about our services, menu, and pricing
- Help customers with booking inquiries
- Provide information about event types we cater
- Collect customer contact information politely

Guidelines:
- Be friendly, professional, and helpful
- If you don't know something, say so and offer to connect them with a human
- Always confirm critical details (date, guest count, location)
- Use tools when appropriate to check availability or create bookings
- Never make up information about pricing or availability

Restaurant Details:
- Closed Mondays
- Serve groups from 10-500 guests
- Offer chicken, steak, shrimp, vegetable hibachi options
- Average price: $25-35 per person
- Require 50% deposit for booking confirmation
```

---

## Testing

### Unit Tests

```python
# tests/test_ai_service.py
import pytest
from app.services.ai_service import AIService

@pytest.fixture
def ai_service():
    return AIService()

async def test_chat_response(ai_service):
    """Test basic chat response"""
    response = await ai_service.chat(
        message="What's the price for 25 people?",
        agent="customer"
    )
    
    assert response.content is not None
    assert "price" in response.content.lower()
    assert response.tokens_used > 0

async def test_tool_calling(ai_service):
    """Test tool calling functionality"""
    response = await ai_service.chat(
        message="I want to book for June 15th, 30 people",
        agent="customer",
        user_context={"name": "John Doe", "phone": "555-123-4567"}
    )
    
    assert response.tool_calls is not None
    assert response.tool_calls[0]["name"] == "create_booking_inquiry"

async def test_rate_limiting(ai_service):
    """Test rate limiting"""
    # Make 11 requests (limit is 10/min)
    for i in range(11):
        if i < 10:
            response = await ai_service.chat(message=f"Test {i}", agent="customer")
            assert response.status_code == 200
        else:
            with pytest.raises(RateLimitExceeded):
                await ai_service.chat(message="Test 11", agent="customer")
```

### Integration Tests

```python
# tests/integration/test_ai_endpoints.py
async def test_chat_endpoint(client):
    """Test chat endpoint"""
    response = await client.post("/api/v1/ai/chat", json={
        "message": "Hi, I want to book a party",
        "agent": "customer"
    })
    
    assert response.status_code == 200
    data = response.json()
    assert "response" in data
    assert "thread_id" in data

async def test_streaming_endpoint(client):
    """Test streaming chat endpoint"""
    response = await client.post("/api/v1/ai/chat/stream", json={
        "message": "Tell me about your services"
    })
    
    assert response.status_code == 200
    assert response.headers["content-type"] == "text/event-stream"
    
    # Consume stream
    tokens = []
    async for line in response.content:
        if line.startswith(b"data:"):
            data = json.loads(line[5:])
            if data["type"] == "token":
                tokens.append(data["content"])
    
    assert len(tokens) > 0
```

---

## Monitoring

### Key Metrics

**Usage Metrics:**
- Total conversations per day
- Messages per conversation
- Average conversation length (tokens)
- Tool calls per conversation

**Performance Metrics:**
- API response time
- Streaming latency
- Token generation rate (tokens/second)

**Cost Metrics:**
- Cost per conversation
- Daily OpenAI spend
- Cost by agent type
- Cost per customer acquisition

**Quality Metrics:**
- Customer satisfaction ratings
- Conversation resolution rate
- Escalation to human rate
- Tool calling success rate

### Grafana Dashboards

**AI Chatbot Overview:**
```yaml
panels:
  - title: "Conversations per Hour"
    query: rate(ai_conversations_total[1h])
    
  - title: "Average Tokens per Conversation"
    query: avg(ai_tokens_per_conversation)
    
  - title: "OpenAI Cost per Hour"
    query: rate(ai_cost_dollars_total[1h])
    
  - title: "Response Time p95"
    query: histogram_quantile(0.95, ai_response_time_seconds)
```

### Alerts

```yaml
alerts:
  - name: HighAICost
    condition: rate(ai_cost_dollars_total[1h]) > 10
    severity: warning
    message: "AI cost exceeding $10/hour"
    
  - name: HighErrorRate
    condition: rate(ai_errors_total[5m]) > 0.1
    severity: critical
    message: "AI error rate > 10%"
    
  - name: SlowResponses
    condition: histogram_quantile(0.95, ai_response_time_seconds) > 5
    severity: warning
    message: "AI responses taking >5 seconds (p95)"
```

---

## Troubleshooting

### Common Issues

**1. "OpenAI API rate limit exceeded"**

**Symptoms:**
- Error message: "Rate limit reached for requests"
- Status code: 429

**Causes:**
- Exceeded OpenAI account rate limits
- Burst of simultaneous requests

**Solutions:**
```python
# Implement exponential backoff
import asyncio
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
async def chat_with_retry(message: str):
    return await openai.chat.completions.create(...)
```

**2. "High token usage / costs"**

**Symptoms:**
- Daily costs higher than expected
- `ai_tokens_per_conversation` metric increasing

**Causes:**
- Verbose system prompts
- Long conversation histories
- Unnecessary context

**Solutions:**
```python
# Trim conversation history
def trim_history(messages: list, max_tokens: int = 4000):
    """Keep only recent messages within token limit"""
    total_tokens = 0
    trimmed = []
    
    for msg in reversed(messages):
        msg_tokens = estimate_tokens(msg["content"])
        if total_tokens + msg_tokens > max_tokens:
            break
        trimmed.insert(0, msg)
        total_tokens += msg_tokens
    
    return trimmed

# Use smaller models when appropriate
if intent == "simple_question":
    model = "gpt-3.5-turbo"  # Cheaper
else:
    model = "gpt-4"  # More capable
```

**3. "Slow streaming responses"**

**Symptoms:**
- Tokens arriving slowly on frontend
- User seeing choppy typing effect

**Causes:**
- Network latency
- Backend processing overhead
- OpenAI API latency

**Solutions:**
```python
# Buffer and batch tokens
async def buffered_stream(stream):
    buffer = []
    last_yield = time.time()
    
    async for chunk in stream:
        buffer.append(chunk)
        
        # Yield every 50ms or 5 tokens
        if time.time() - last_yield > 0.05 or len(buffer) >= 5:
            yield ''.join(buffer)
            buffer = []
            last_yield = time.time()
    
    if buffer:
        yield ''.join(buffer)
```

---

## Related Documentation

- [API_DOCUMENTATION.md](../API_DOCUMENTATION.md) - API endpoints
- [TESTING_COMPREHENSIVE_GUIDE.md](../TESTING_COMPREHENSIVE_GUIDE.md) - Testing procedures
- [PRODUCTION_OPERATIONS_RUNBOOK.md](../PRODUCTION_OPERATIONS_RUNBOOK.md) - Production operations

---

## Consolidated Documents

This guide consolidates:
- AI_CHAT_FIXES_COMPLETE.md
- CHATBOT_CONTACT_COLLECTION_COMPLETE.md
- CHATBOT_NAME_COLLECTION_IMPLEMENTATION.md
- AGENT_AWARE_AI_IMPLEMENTATION_COMPLETE.md (archived)
- AI_COMPREHENSIVE_AUDIT_REPORT.md

---

**Last Updated:** October 25, 2025  
**Maintained By:** Development Team  
**Version:** 2.1.0

---
