/**
 * Performance Benchmarks for Cache System
 *
 * Benchmarks:
 * - Cache hit vs miss response times
 * - Memory usage with/without caching
 * - Cache hit rate over time
 * - Storage overhead
 * - Concurrent request handling
 */

import { describe, it, expect, beforeEach, vi } from 'vitest'

import { getCacheService, resetCacheService } from '../cacheService'

describe('Cache Performance Benchmarks', () => {
  beforeEach(() => {
    resetCacheService()
  })

  describe('Response Time Benchmarks', () => {
    it('should show cache hit is significantly faster than miss', async () => {
      const cacheService = getCacheService()
      const slowFetcher = async () => {
        // Simulate slow API call (100ms)
        await new Promise(resolve => setTimeout(resolve, 100))
        return { data: 'slow response' }
      }

      // Cache miss - first call (slow)
      const missStart = performance.now()
      const missResult = await cacheService.cacheFirst('benchmark-key', 60000, slowFetcher)
      const missDuration = performance.now() - missStart

      // Cache hit - second call (fast)
      const hitStart = performance.now()
      const hitResult = await cacheService.cacheFirst('benchmark-key', 60000, slowFetcher)
      const hitDuration = performance.now() - hitStart

      expect(missResult).toEqual(hitResult)
      expect(missDuration).toBeGreaterThan(100) // At least 100ms (network delay)
      expect(hitDuration).toBeLessThan(10) // Sub-10ms (memory access)

      console.log(`Cache Miss: ${missDuration.toFixed(2)}ms`)
      console.log(`Cache Hit: ${hitDuration.toFixed(2)}ms`)
      console.log(`Speedup: ${(missDuration / hitDuration).toFixed(1)}x faster`)
    })

    it('should measure response time improvements across multiple calls', async () => {
      const cacheService = getCacheService()
      const times: number[] = []

      const fetcher = async () => {
        await new Promise(resolve => setTimeout(resolve, 50))
        return { timestamp: Date.now() }
      }

      // Make 10 calls and measure each
      for (let i = 0; i < 10; i++) {
        const start = performance.now()
        await cacheService.cacheFirst('speed-test', 60000, fetcher)
        times.push(performance.now() - start)
      }

      // First call should be slow (miss)
      expect(times[0]).toBeGreaterThan(50)

      // Subsequent calls should be fast (hits)
      const avgCachedTime = times.slice(1).reduce((a, b) => a + b, 0) / (times.length - 1)
      expect(avgCachedTime).toBeLessThan(10)

      console.log('Response Times:')
      console.log(`  First call (miss): ${times[0].toFixed(2)}ms`)
      console.log(`  Avg cached calls: ${avgCachedTime.toFixed(2)}ms`)
      console.log(`  Min cached: ${Math.min(...times.slice(1)).toFixed(2)}ms`)
      console.log(`  Max cached: ${Math.max(...times.slice(1)).toFixed(2)}ms`)
    })
  })

  describe('Cache Hit Rate Benchmarks', () => {
    it('should achieve high hit rate with repeated requests', async () => {
      const cacheService = getCacheService()
      const fetcher = vi.fn(async () => ({ data: 'test' }))

      // Simulate realistic usage: 100 requests across 10 keys
      const keys = Array.from({ length: 10 }, (_, i) => `key-${i}`)

      for (let i = 0; i < 100; i++) {
        const randomKey = keys[Math.floor(Math.random() * keys.length)]
        await cacheService.cacheFirst(randomKey, 60000, fetcher)
      }

      const hitRate = cacheService.getHitRate()
      const metadata = cacheService.getMetadata()

      console.log('Cache Hit Rate Benchmark:')
      console.log(`  Total Requests: ${metadata.hits + metadata.misses}`)
      console.log(`  Cache Hits: ${metadata.hits}`)
      console.log(`  Cache Misses: ${metadata.misses}`)
      console.log(`  Hit Rate: ${(hitRate * 100).toFixed(1)}%`)

      // With 10 keys and 100 requests, we expect 90% hit rate
      expect(hitRate).toBeGreaterThan(0.85) // At least 85%
    })

    it('should track hit rate improvement over time', async () => {
      const cacheService = getCacheService()
      const fetcher = async () => ({ data: 'test' })
      const hitRates: number[] = []

      // Make requests and track hit rate every 10 requests
      for (let i = 0; i < 50; i++) {
        await cacheService.cacheFirst('improvement-key', 60000, fetcher)

        if (i % 10 === 9) {
          hitRates.push(cacheService.getHitRate())
        }
      }

      console.log('Hit Rate Over Time:')
      hitRates.forEach((rate, idx) => {
        console.log(`  After ${(idx + 1) * 10} requests: ${(rate * 100).toFixed(1)}%`)
      })

      // Hit rate should increase over time
      expect(hitRates[hitRates.length - 1]).toBeGreaterThan(hitRates[0])
    })
  })

  describe('Memory Usage Benchmarks', () => {
    it('should measure cache memory overhead', () => {
      const cacheService = getCacheService()

      // Add 100 entries with realistic data
      for (let i = 0; i < 100; i++) {
        cacheService.set(`entry-${i}`, {
          id: i,
          name: `Item ${i}`,
          description: 'A'.repeat(100), // ~100 bytes
          metadata: {
            created: new Date().toISOString(),
            tags: ['tag1', 'tag2', 'tag3'],
          },
        }, 60000)
      }

      const metadata = cacheService.getMetadata()

      console.log('Memory Usage Benchmark:')
      console.log(`  Entries: ${metadata.entries}`)
      console.log(`  Total Size: ${(metadata.size / 1024).toFixed(2)} KB`)
      console.log(`  Avg Entry Size: ${(metadata.size / metadata.entries).toFixed(0)} bytes`)

      // Verify reasonable memory usage (should be < 100KB for 100 entries)
      expect(metadata.size).toBeLessThan(100 * 1024) // < 100KB
    })

    it('should enforce size limits with LRU eviction', () => {
      resetCacheService()
      const smallCache = getCacheService({ maxSize: 5000 }) // 5KB limit

      const largeEntry = { data: 'x'.repeat(1000) } // ~1KB each

      // Add 10 entries (should evict oldest ones)
      for (let i = 0; i < 10; i++) {
        smallCache.set(`large-${i}`, largeEntry, 60000)
      }

      const metadata = smallCache.getMetadata()

      console.log('LRU Eviction Benchmark:')
      console.log(`  Max Size: 5KB`)
      console.log(`  Attempted: 10 entries (~10KB)`)
      console.log(`  Actual Entries: ${metadata.entries}`)
      console.log(`  Actual Size: ${(metadata.size / 1024).toFixed(2)} KB`)

      // Should stay under limit
      expect(metadata.size).toBeLessThanOrEqual(5000)

      // Should have evicted some entries
      expect(metadata.entries).toBeLessThan(10)

      // Oldest entries should be gone
      expect(smallCache.get('large-0')).toBeNull()

      // Newest entries should exist
      expect(smallCache.get('large-9')).toBeTruthy()
    })
  })

  describe('Concurrent Request Benchmarks', () => {
    it('should handle concurrent requests efficiently', async () => {
      const cacheService = getCacheService()
      let fetchCount = 0

      const fetcher = async () => {
        fetchCount++
        await new Promise(resolve => setTimeout(resolve, 50))
        return { value: 'data' }
      }

      // Launch 20 concurrent requests for same key
      const start = performance.now()
      const promises = Array.from({ length: 20 }, () =>
        cacheService.cacheFirst('concurrent-key', 60000, fetcher)
      )
      await Promise.all(promises)
      const duration = performance.now() - start

      console.log('Concurrent Request Benchmark:')
      console.log(`  Concurrent Requests: 20`)
      console.log(`  Total Duration: ${duration.toFixed(2)}ms`)
      console.log(`  Fetch Calls: ${fetchCount}`)

      // Without caching, this would take 20 * 50ms = 1000ms
      // With caching, should be much faster after first fetch
      expect(duration).toBeLessThan(500) // Under 500ms

      // Depending on race conditions, fetch might be called 1-20 times
      // Ideally once, but some concurrent requests might start before cache is set
      console.log(`  Cache Efficiency: Reduced ${20 - fetchCount} fetches`)
    })

    it('should handle concurrent requests for different keys', async () => {
      const cacheService = getCacheService()
      const fetchTimes: { [key: string]: number } = {}

      const fetcher = async (key: string) => {
        const start = performance.now()
        await new Promise(resolve => setTimeout(resolve, 30))
        fetchTimes[key] = performance.now() - start
        return { key, data: 'test' }
      }

      // Launch concurrent requests for different keys
      const keys = Array.from({ length: 10 }, (_, i) => `key-${i}`)
      const start = performance.now()

      await Promise.all(
        keys.map(key =>
          cacheService.cacheFirst(key, 60000, () => fetcher(key))
        )
      )

      const totalDuration = performance.now() - start

      console.log('Concurrent Different Keys Benchmark:')
      console.log(`  Keys: ${keys.length}`)
      console.log(`  Total Duration: ${totalDuration.toFixed(2)}ms`)
      console.log(`  Expected Sequential: ${keys.length * 30}ms`)
      console.log(`  Speedup: ${(keys.length * 30 / totalDuration).toFixed(1)}x`)

      // With concurrent execution, should be much faster than sequential
      expect(totalDuration).toBeLessThan(keys.length * 30)
    })
  })

  describe('Strategy Comparison Benchmarks', () => {
    it('should compare all 3 cache strategies performance', async () => {
      const cacheService = getCacheService()
      const results: { [strategy: string]: { miss: number; hit: number } } = {
        'cache-first': { miss: 0, hit: 0 },
        'stale-while-revalidate': { miss: 0, hit: 0 },
        'network-first': { miss: 0, hit: 0 },
      }

      const fetcher = async () => {
        await new Promise(resolve => setTimeout(resolve, 50))
        return { data: 'test' }
      }

      // Test cache-first
      const cf1Start = performance.now()
      await cacheService.cacheFirst('cf-key', 60000, fetcher)
      results['cache-first'].miss = performance.now() - cf1Start

      const cf2Start = performance.now()
      await cacheService.cacheFirst('cf-key', 60000, fetcher)
      results['cache-first'].hit = performance.now() - cf2Start

      // Test stale-while-revalidate
      const swr1Start = performance.now()
      await cacheService.staleWhileRevalidate('swr-key', 60000, fetcher)
      results['stale-while-revalidate'].miss = performance.now() - swr1Start

      const swr2Start = performance.now()
      await cacheService.staleWhileRevalidate('swr-key', 60000, fetcher)
      results['stale-while-revalidate'].hit = performance.now() - swr2Start

      // Test network-first
      const nf1Start = performance.now()
      await cacheService.networkFirst('nf-key', 60000, fetcher)
      results['network-first'].miss = performance.now() - nf1Start

      const nf2Start = performance.now()
      await cacheService.networkFirst('nf-key', 60000, fetcher)
      results['network-first'].hit = performance.now() - nf2Start

      console.log('Strategy Performance Comparison:')
      Object.entries(results).forEach(([strategy, times]) => {
        console.log(`  ${strategy}:`)
        console.log(`    Miss: ${times.miss.toFixed(2)}ms`)
        console.log(`    Hit: ${times.hit.toFixed(2)}ms`)
      })

      // cache-first hit should be fastest
      expect(results['cache-first'].hit).toBeLessThan(
        results['network-first'].hit
      )

      // network-first should always try network (slower hits)
      expect(results['network-first'].hit).toBeGreaterThan(
        results['cache-first'].hit
      )
    })
  })

  describe('Storage Overhead Benchmarks', () => {
    it('should measure localStorage sync overhead', () => {
      const cacheService = getCacheService()
      const entries = 100

      // Measure time to set entries (includes localStorage sync)
      const start = performance.now()
      for (let i = 0; i < entries; i++) {
        cacheService.set(`storage-${i}`, { data: `value-${i}` }, 60000)
      }
      const duration = performance.now() - start

      console.log('localStorage Sync Benchmark:')
      console.log(`  Entries: ${entries}`)
      console.log(`  Total Time: ${duration.toFixed(2)}ms`)
      console.log(`  Avg per Entry: ${(duration / entries).toFixed(2)}ms`)

      // Should be reasonably fast (< 10ms per entry)
      expect(duration / entries).toBeLessThan(10)
    })

    it('should measure cache retrieval from localStorage', () => {
      const cacheService = getCacheService()

      // Pre-populate cache and localStorage
      for (let i = 0; i < 50; i++) {
        cacheService.set(`retrieve-${i}`, { data: `value-${i}` }, 60000)
      }

      // Create new cache instance (forces localStorage read)
      resetCacheService()
      const newCache = getCacheService()

      // Measure retrieval time
      const start = performance.now()
      for (let i = 0; i < 50; i++) {
        newCache.get(`retrieve-${i}`)
      }
      const duration = performance.now() - start

      console.log('localStorage Retrieval Benchmark:')
      console.log(`  Entries: 50`)
      console.log(`  Total Time: ${duration.toFixed(2)}ms`)
      console.log(`  Avg per Entry: ${(duration / 50).toFixed(2)}ms`)

      // Should be fast (localStorage is synchronous but efficient)
      expect(duration).toBeLessThan(100) // Under 100ms total
    })
  })

  describe('Real-World Scenario Benchmarks', () => {
    it('should simulate booking page with multiple cached endpoints', async () => {
      const cacheService = getCacheService()
      const endpoints = [
        { key: 'booked-dates', latency: 80 },
        { key: 'availability', latency: 60 },
        { key: 'menu', latency: 40 },
        { key: 'pricing', latency: 50 },
      ]

      // First load (all cache misses)
      const firstLoadStart = performance.now()
      await Promise.all(
        endpoints.map(({ key, latency }) =>
          cacheService.cacheFirst(key, 180000, async () => {
            await new Promise(resolve => setTimeout(resolve, latency))
            return { data: key }
          })
        )
      )
      const firstLoadDuration = performance.now() - firstLoadStart

      // Second load (all cache hits)
      const secondLoadStart = performance.now()
      await Promise.all(
        endpoints.map(({ key, latency }) =>
          cacheService.cacheFirst(key, 180000, async () => {
            await new Promise(resolve => setTimeout(resolve, latency))
            return { data: key }
          })
        )
      )
      const secondLoadDuration = performance.now() - secondLoadStart

      console.log('Real-World Booking Page Scenario:')
      console.log(`  Endpoints: ${endpoints.length}`)
      console.log(`  First Load (uncached): ${firstLoadDuration.toFixed(2)}ms`)
      console.log(`  Second Load (cached): ${secondLoadDuration.toFixed(2)}ms`)
      console.log(`  Improvement: ${(firstLoadDuration / secondLoadDuration).toFixed(1)}x faster`)

      // Second load should be dramatically faster
      expect(secondLoadDuration).toBeLessThan(firstLoadDuration * 0.2) // At least 5x faster
    })
  })

  describe('Summary Report', () => {
    it('should generate performance summary', async () => {
      const cacheService = getCacheService()
      const fetcher = async () => {
        await new Promise(resolve => setTimeout(resolve, 30))
        return { data: 'test' }
      }

      // Simulate realistic usage
      for (let i = 0; i < 50; i++) {
        await cacheService.cacheFirst('summary-key', 60000, fetcher)
      }

      const metadata = cacheService.getMetadata()
      const hitRate = cacheService.getHitRate()

      console.log('\n=== CACHE PERFORMANCE SUMMARY ===')
      console.log(`Total Requests: ${metadata.hits + metadata.misses}`)
      console.log(`Cache Hits: ${metadata.hits} (${(hitRate * 100).toFixed(1)}%)`)
      console.log(`Cache Misses: ${metadata.misses}`)
      console.log(`Entries in Cache: ${metadata.entries}`)
      console.log(`Cache Size: ${(metadata.size / 1024).toFixed(2)} KB`)
      console.log(`Estimated Time Saved: ${(metadata.hits * 30).toFixed(0)}ms`)
      console.log('================================\n')

      // Verify good performance
      expect(hitRate).toBeGreaterThan(0.95) // >95% hit rate with single key
    })
  })
})
